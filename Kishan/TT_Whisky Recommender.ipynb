{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e63398df",
   "metadata": {},
   "source": [
    "# Whisky Recommender\n",
    "<b>Tentative Workflow</b>\n",
    "1. Extract Nose, Taste, Finish text\n",
    "2. Combine text from Step into 1 column\n",
    "3. Remove punctuations and stopwords from output of Step 2\n",
    "4. Tokenize output of Step 3\n",
    "5. Lemmatize output of Step 4\n",
    "6. Create W2V vectors on output of Step 5\n",
    "7. Create W2V vector for a test review\n",
    "8. Print the title of the vector in Step 6 that is closest to the vector in Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6623409d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26831180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('max_colwidth', 2000)\n",
    "\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebe5ac2",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ce6b170",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'clean_whisky.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_whisky.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# For Machine Learning, we would only need the Title and Combined Text columns\u001b[39;00m\n\u001b[0;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNose\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTaste\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinish\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'clean_whisky.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"clean_whisky.csv\")\n",
    "\n",
    "# For Machine Learning, we would only need the Title and Combined Text columns\n",
    "data = data[['Title', 'Nose', 'Taste', 'Finish']]\n",
    "\n",
    "# This ensures that even if there are non-string values in the columns, they will be converted to strings before concatenation.\n",
    "data['combined_text'] = data['Nose'].astype(str) + \" \" + data['Taste'].astype(str) + \" \" + data['Finish'].astype(str)\n",
    "\n",
    "# Check if the text gets combined correctly for each review\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904e7f95",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df506fc",
   "metadata": {},
   "source": [
    "### Remove Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73034efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# string.punctuation comprises of a list of all punctuations\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcd067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    # store character only if it is not a punctuation\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "data['combined_text_clean'] = data['combined_text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22e2e56",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fbfb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # Matches any character that is neither alphanumeric nor underscore\n",
    "    # Add a + just in case there are 2 or more spaces between certain words\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "data['combined_text_tokenized'] = data['combined_text_clean'].apply(lambda x: tokenize(x.lower()))\n",
    "\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a74a236",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733cf21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# NLTK(Natural Language Toolkit) in python has a list of stopwords stored in 16 different languages.\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# if you get an error with the above code, comment the code above and run this & follow below directions:\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce8faaf",
   "metadata": {},
   "source": [
    "<b>If you ran into issues with the above:</b>\n",
    "\n",
    "1. Run <code>nltk.download()</code>. A new screen will pop up outside your Jupyter notebook. (It may be hidden behind other windows.)\n",
    "2. Once this box opens up, click <code>all</code>, then <code>download</code>. Once this is done, restart your Jupyter notebook and try running the cell above again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2aeace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_list):\n",
    "    # Store in text only if word is not found in stopword i.e. it is not a stopword\n",
    "    text = [word for word in tokenized_list if word not in stopword]\n",
    "    return text\n",
    "\n",
    "data['combined_text_nostop'] = data['combined_text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124b1d93",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ed82e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you get an error with this cell, uncomment the line of code and re-run this cell\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizing(tokenized_text):\n",
    "    # return list of all lemmatized words for their corresponding words in tokenized_text\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "data['combined_text_lemmatized'] = data['combined_text_nostop'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c4363e",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6330897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the word2vec model\n",
    "# vector_size = size of word vector. size input all dimensions that meet window param value\n",
    "# window = number of words before and after the focus word that will be considered as context\n",
    "# min_count = number of times word must appear in corpus in order to create a word vector\n",
    "### Important - Model will only be trained on words that meet min_count so it may not learn all words as a result\n",
    "w2v_model = Word2Vec(data[\"combined_text_lemmatized\"], vector_size=100, window=5, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cd28b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list to store the average of all word vectors for each row of data\n",
    "review_vect_list = []\n",
    "\n",
    "# Iterate through each row in X train data\n",
    "for index, row in data.iterrows():\n",
    "\n",
    "    # Create a Word2Vec vector for each row (review) in train data \n",
    "    # By applying the W2V model on each word of each review, we get the word vector (embedding) for each word\n",
    "    # Each row (review) will be represented by average of all vectors of all words in each row that model has trained on\n",
    "    model_vector = (np.mean([w2v_model.wv.get_vector(token) for token in row['combined_text_lemmatized'] if token in w2v_model.wv], axis=0)).tolist()\n",
    "\n",
    "    # Check if the line exists else it is vector of zeros\n",
    "    if type(model_vector) is list:  \n",
    "        review_vect_list.append(model_vector)\n",
    "    else:\n",
    "        review_vect_list.append([str(0) for i in range(100)])\n",
    "        \n",
    "word2vec_df = pd.DataFrame(review_vect_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1526496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'sentence_vector' column to data DataFrame\n",
    "data['sentence_vector'] = word2vec_df.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255bf11",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8d8ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get word vectors\n",
    "def get_word_vectors(sentence):\n",
    "    word_vectors = []\n",
    "    for word in sentence.split():\n",
    "        if word in w2v_model.wv:\n",
    "            word_vectors.append(w2v_model.wv[word])\n",
    "    return word_vectors\n",
    "\n",
    "# Function to calculate sentence vector by averaging word vectors\n",
    "def get_sentence_vector(sentence):\n",
    "    word_vectors = get_word_vectors(sentence)\n",
    "    if not word_vectors:\n",
    "        return np.zeros(w2v_model.vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Function to find the closest sentence to the input text and calculate the cosine similarity percentage\n",
    "def find_closest_sentence(input_text, data):\n",
    "    input_vector = get_sentence_vector(input_text)\n",
    "    max_similarity = -1\n",
    "    closest_sentence = None\n",
    "    closest_similarity_percent = 0\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        sentence_vector = row['sentence_vector']\n",
    "        similarity = cosine_similarity([input_vector], [sentence_vector])[0][0]\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            closest_sentence = row['Title']\n",
    "            closest_similarity_percent = similarity  # Save the similarity percentage\n",
    "\n",
    "    return closest_sentence, closest_similarity_percent\n",
    "\n",
    "    # Convert cosine similarity to percentage\n",
    "    closest_similarity_percent = \"{:.2f}\".format(closest_similarity * 100)\n",
    "    \n",
    "    return closest_sentence, closest_similarity_percent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ad390c-250a-4bf2-a1c2-235ac1613bc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the food choices and descriptions\n",
    "food_choices = {\n",
    "    \"Hainanese Chicken Rice\": \"Hainanese chicken rice is a beloved dish consisting of tender poached chicken served with flavorful rice, accompanied by chili sauce and ginger paste. The chicken is typically succulent and infused with subtle aromas, while the rice is rich in flavor from being cooked in chicken broth.\",\n",
    "    \"Laksa\": \"Laksa is a spicy noodle soup with a rich and creamy coconut curry broth. It's typically served with rice noodles and topped with shrimp, fish cakes, bean sprouts, and a hard-boiled egg. The broth is flavorful and aromatic, with a perfect balance of spice and creaminess.\",\n",
    "    \"Char Kway Teow\": \"Char Kway Teow is a popular stir-fried noodle dish known for its smoky flavor and savory sauce. It features flat rice noodles cooked with Chinese sausage, shrimp, cockles, bean sprouts, and chives, all stir-fried in a flavorful dark soy sauce-based seasoning.\",\n",
    "    \"Chilli Crab\": \"Chilli Crab is a quintessential Singaporean seafood dish featuring mud crab cooked in a tangy and spicy chili sauce. The sauce is rich and flavorful, with hints of sweetness from the tomato paste and a kick of heat from the chili. It's often enjoyed with mantou (fried buns) for dipping.\",\n",
    "    \"Satay\": \"Satay is a popular street food in Singapore consisting of skewered and grilled meat served with a flavorful peanut sauce. The meat is marinated in a blend of spices, then grilled to perfection, resulting in tender and juicy skewers. It's often served with cucumber slices and rice cakes.\",\n",
    "    \"Roti Prata\": \"Roti Prata is a crispy and flaky flatbread that originated from Indian cuisine but has become a favorite in Singapore. It's typically served with a side of savory curry dipping sauce, which complements the buttery and crispy texture of the prata.\",\n",
    "    \"Nasi Lemak\": \"Nasi Lemak is a traditional Malay dish known for its fragrant coconut rice served with an array of flavorful accompaniments. It's often accompanied by fried chicken or fish, sambal chili for heat, fried anchovies and roasted peanuts for crunch, cucumber slices for freshness, and a hard-boiled egg for protein.\",\n",
    "    \"Mee Goreng\": \"Mee Goreng is a flavorful and spicy stir-fried noodle dish with origins in Indian and Malay cuisine. It features yellow noodles stir-fried with a variety of vegetables, tofu, shrimp, and a spicy and tangy sauce made from a blend of chili, tomato, and spices.\",\n",
    "    \"Bak Kut Teh\": \"Bak Kut Teh, which translates to \\\"meat bone tea,\\\" is a comforting soup dish consisting of tender pork ribs simmered in a flavorful broth of Chinese herbs and spices. The broth is rich and aromatic, with layers of flavor from ingredients like garlic, pepper, and star anise.\",\n",
    "    \"Hokkien Mee\": \"Hokkien Mee is a popular noodle dish in Singapore made from a mix of yellow and rice noodles stir-fried with seafood, such as shrimp and squid, and vegetables like cabbage and bean sprouts. The dish is infused with a rich seafood flavor from the broth, resulting in a hearty and satisfying meal.\",\n",
    "    \"Bak Chor Mee\": \"Bak Chor Mee is a flavorful noodle dish featuring springy egg noodles tossed in a spicy vinegar sauce and topped with minced pork, pork slices, mushrooms, and meatballs. The combination of savory, tangy, and spicy flavors makes it a favorite among noodle lovers.\",\n",
    "    \"Murtabak\": \"Murtabak is a savory pancake of Arab origin that has become popular in Singapore. It's made from thin, flaky pastry filled with a mixture of minced meat, onions, and egg, then folded and grilled to perfection. It's often served with a side of curry dipping sauce for added flavor.\",\n",
    "    \"Kaya Toast\": \"Kaya Toast is a classic Singaporean breakfast dish consisting of toasted bread spread with kaya, a sweet and creamy coconut and egg jam, and butter. It's typically served with soft-boiled eggs and a cup of coffee or tea, making it a comforting and delicious start to the day.\",\n",
    "    \"Rojak\": \"Rojak is a traditional fruit and vegetable salad dish popular in Singapore and Malaysia. It features a mix of crunchy fruits and vegetables tossed in a sweet and spicy shrimp paste sauce, then topped with crushed peanuts and sesame seeds for added texture and flavor.\",\n",
    "    \"Nasi Padang\": \"Nasi Padang is an Indonesian rice dish that has become popular in Singapore. It consists of steamed rice served with a variety of flavorful and spicy dishes, such as rendang, sambal goreng, and sayur lodeh. It's a feast for the senses, with a mix of bold flavors and aromatic spices.\",\n",
    "    \"Biryani\": \"Biryani is a fragrant and flavorful rice dish of South Asian origin that has become popular in Singapore. It features long-grain basmati rice cooked with a blend of aromatic spices, layered with tender meat (chicken, mutton, or beef), and served with a side of cooling yogurt-based raita.\",\n",
    "    \"Otah-Otah\": \"Otah-Otah is a traditional Malay dish made from spicy fish paste mixed with herbs and spices, then wrapped in banana leaves and grilled or steamed until cooked through. It's a flavorful and aromatic dish with a perfect balance of spice and sweetness from the banana leaves.\",\n",
    "    \"Kueh Pie Tee\": \"Kueh Pie Tee is a popular Peranakan snack in Singapore made from crispy pastry cups filled with a savory mixture of shredded turnips, carrots, and prawns, then topped with chili sauce and fried shallots. It's a delightful combination of textures and flavors, with a hint of sweetness from the turnips.\",\n",
    "    \"Tau Huay\": \"Tau Huay, also known as tofu pudding or douhua, is a comforting dessert popular in Singapore. It's made from soft and silky smooth tofu pudding served with a sweet syrup, often flavored with pandan leaves or ginger for added fragrance. It's a light and refreshing dessert perfect for a hot day.\",\n",
    "    \"Chwee Kueh\": \"Chwee Kueh is a classic Teochew snack in Singapore made from steamed rice cakes topped with a savory mixture of preserved radish (chye poh) and fried shallots. It's typically served with a side of spicy chili sauce for added flavor and heat. The soft and slightly chewy texture of the rice cakes contrasts beautifully with the crunchy toppings.\",\n",
    "    \"Mee Siam\": \"Mee Siam is a flavorful noodle dish with Malay and Thai influences, popular in Singapore. It features rice vermicelli noodles cooked in a spicy, sweet, and tangy gravy, served with prawns, tofu, hard-boiled egg, and bean sprouts. It's a vibrant and satisfying dish with a perfect balance of flavors.\",\n",
    "    \"Kway Chap\": \"Kway Chap is a comforting noodle soup dish of Teochew origin popular in Singapore. It features wide rice noodles served in a rich and aromatic broth made from pork bones and various spices, accompanied by tender braised pork, tofu, hard-boiled eggs, and offal. It's a hearty and flavorful meal that's perfect for any time of the day.\",\n",
    "    \"Putu Piring\": \"Putu Piring is a traditional Malay dessert in Singapore made from steamed rice flour cakes filled with gooey gula melaka (palm sugar) and served with freshly grated coconut. It's a sweet and fragrant treat with a melt-in-your-mouth texture, perfect for satisfying your sweet tooth.\",\n",
    "    \"Bak Kwa\": \"Bak Kwa is a popular Chinese New Year snack in Singapore made from grilled slices of sweet and savory dried meat, typically pork or beef. The meat is thinly sliced and marinated in a blend of spices and sugar before being grilled to perfection, resulting in a tender and flavorful snack that's irresistible.\",\n",
    "    \"Ice Kachang\": \"Ice Kachang is a beloved dessert in Singapore made from shaved ice topped with colorful syrup, sweetened condensed milk, red beans, attap chee (palm seed), grass jelly, and corn. It's a refreshing and sweet treat that's perfect for cooling down on a hot day.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cefa365-4167-4808-a86a-981c9aabb437",
   "metadata": {},
   "source": [
    "# Function to display food choices and get user input\n",
    "def select_food_choice():\n",
    "    print(\"Select a food choice:\")\n",
    "    for index, (food, description) in enumerate(food_choices.items(), start=1):\n",
    "        print(f\"{index}. {food} - {description}\")\n",
    "\n",
    "    # Get user choice\n",
    "    choice_index = input(\"Enter the number of your choice: \")\n",
    "    try:\n",
    "        choice_index = int(choice_index)\n",
    "        if 1 <= choice_index <= len(food_choices):\n",
    "            return list(food_choices.keys())[choice_index - 1]\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter a number between 1 and\", len(food_choices))\n",
    "            return select_food_choice()  # Ask again for valid input\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a number.\")\n",
    "        return select_food_choice()  # Ask again for valid input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839c6dd-4df7-46b9-b84d-cb18c8cba613",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to display food choices and get user input\n",
    "def select_food_choices(num_choices=3):\n",
    "    print(\"Select\", num_choices, \"food choices:\")\n",
    "    selected_foods = []\n",
    "    for i in range(num_choices):\n",
    "        print(\"Choice\", i+1)\n",
    "        for index, (food, description) in enumerate(food_choices.items(), start=1):\n",
    "            print(f\"{index}. {food} - {description}\")\n",
    "\n",
    "        # Get user choice\n",
    "        choice_index = input(\"Enter the number of your choice: \")\n",
    "        try:\n",
    "            choice_index = int(choice_index)\n",
    "            if 1 <= choice_index <= len(food_choices):\n",
    "                selected_foods.append(list(food_choices.keys())[choice_index - 1])\n",
    "            else:\n",
    "                print(\"Invalid choice. Please enter a number between 1 and\", len(food_choices))\n",
    "                return select_food_choices(num_choices)  # Ask again for valid input\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a number.\")\n",
    "            return select_food_choices(num_choices)  # Ask again for valid input\n",
    "    return selected_foods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7938ad92-bb23-402a-a2c4-80ead7c92e41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Welcome message and food choices\n",
    "print(\"Welcome to the food recommender system!\")\n",
    "print(\"Select 3 food choices:\")\n",
    "\n",
    "# Display food choices\n",
    "for i, food in enumerate(food_choices.keys(), start=1):\n",
    "    print(f\"{i}. {food} - {food_choices[food]}\")\n",
    "\n",
    "# Prompt user to choose 3 foods\n",
    "selected_foods = []\n",
    "while len(selected_foods) < 3:\n",
    "    choice = input(\"\\nEnter the number of your choice: \")\n",
    "    if choice.isdigit():\n",
    "        choice = int(choice)\n",
    "        if 1 <= choice <= len(food_choices):\n",
    "            food = list(food_choices.keys())[choice - 1]\n",
    "            if food not in selected_foods:\n",
    "                selected_foods.append(food)\n",
    "            else:\n",
    "                print(\"You have already selected this food. Please choose a different one.\")\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter a number within the range.\")\n",
    "    else:\n",
    "        print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "# Find the closest review for each selected food\n",
    "closest_food = None\n",
    "max_similarity_percent = -1\n",
    "\n",
    "for food in selected_foods:\n",
    "    food_description = food_choices[food]\n",
    "    closest_sentence, closest_similarity_percent = find_closest_sentence(food_description, data)\n",
    "    print(\"\\nFor\", food, \":\")\n",
    "    print(\"Closest Review:\", closest_sentence)\n",
    "    print(\"Cosine Similarity Percentage:\", format(closest_similarity_percent, '.2%'))\n",
    "    \n",
    "    # Keep track of the food with the highest similarity percentage\n",
    "    if closest_similarity_percent > max_similarity_percent:\n",
    "        max_similarity_percent = closest_similarity_percent\n",
    "        closest_food = food\n",
    "\n",
    "# Combine descriptions of selected foods\n",
    "combined_description = \" \".join([food_choices[food] for food in selected_foods])\n",
    "\n",
    "# Find closest review based on combined description\n",
    "closest_review = None\n",
    "max_similarity_percent_combined = -1\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    sentence_vector = row['sentence_vector']\n",
    "    similarity = cosine_similarity([get_sentence_vector(combined_description)], [sentence_vector])[0][0]\n",
    "    if similarity > max_similarity_percent_combined:\n",
    "        max_similarity_percent_combined = similarity\n",
    "        closest_review = row['Title']\n",
    "\n",
    "# Display the food with the highest similarity percentage and closest review based on selected foods\n",
    "print(\"\\nFood with the highest similarity percentage:\", closest_food)\n",
    "print(\"\\nClosest Review based on selected foods:\")\n",
    "print(\"Review:\", closest_review)\n",
    "print(\"Cosine Similarity Percentage:\", format(max_similarity_percent_combined, '.2%'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b0860b-48c3-42e9-9ed9-113ef344eaa1",
   "metadata": {},
   "source": [
    "# Get description of selected foods and find closest sentence for each\n",
    "for food in selected_foods:\n",
    "    food_description = food_choices[food]\n",
    "    closest_sentence, closest_similarity_percent = find_closest_sentence(food_description, data)\n",
    "    print(\"\\nFor\", food, \":\")\n",
    "    print(\"Closest Review:\", closest_sentence)\n",
    "    print(\"Cosine Similarity Percentage:\", format(closest_similarity_percent, '.2%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460672b4-3888-49a2-821a-441700834a6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Example usage:\n",
    "print(\"Welcome to the food recommender system!\")\n",
    "selected_food = select_food_choice()\n",
    "print(\"Selected food:\", selected_food)\n",
    "\n",
    "# Get description of selected food and find closest sentence\n",
    "food_description = food_choices[selected_food]\n",
    "closest_sentence, closest_similarity_percent = find_closest_sentence(food_description, data)\n",
    "print(\"Closest Review:\", closest_sentence)\n",
    "print(\"Cosine Similarity Percentage:\", closest_similarity_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5727a8e-b594-4c6f-937c-d16fc4ddec0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Example usage:\n",
    "print(\"Enter your preference\")\n",
    "input_text = input()\n",
    "\n",
    "closest_sentence = find_closest_sentence(input_text, data)\n",
    "print(\"Closest Review:\", closest_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
